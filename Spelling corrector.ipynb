{
 "metadata": {
  "name": "Spelling corrector"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I recently came across [this article](http://norvig.com/spell-correct.html) by Peter Norvig. It is a very interesting enquiry into the world of spelling correction. In the spirit of learning by doing, I'm going to try to deconstruct what he does and then reuse it and experiment with it inside this notebook. First of all, let's paste the complete code by Peter Norvig in the next cell."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re, collections\n",
      "\n",
      "def words(text): return re.findall('[a-z]+', text.lower()) \n",
      "\n",
      "def train(features):\n",
      "    model = collections.defaultdict(lambda: 1)\n",
      "    for f in features:\n",
      "        model[f] += 1\n",
      "    return model\n",
      "\n",
      "NWORDS = train(words(file('big.txt').read()))\n",
      "\n",
      "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
      "\n",
      "def edits1(word):\n",
      "   splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
      "   deletes    = [a + b[1:] for a, b in splits if b]\n",
      "   transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]\n",
      "   replaces   = [a + c + b[1:] for a, b in splits for c in alphabet if b]\n",
      "   inserts    = [a + c + b     for a, b in splits for c in alphabet]\n",
      "   return set(deletes + transposes + replaces + inserts)\n",
      "\n",
      "def known_edits2(word):\n",
      "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS)\n",
      "\n",
      "def known(words): return set(w for w in words if w in NWORDS)\n",
      "\n",
      "def correct(word):\n",
      "    candidates = known([word]) or known(edits1(word)) or known_edits2(word) or [word]\n",
      "    return max(candidates, key=NWORDS.get)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IOError",
       "evalue": "[Errno 2] No such file or directory: 'big.txt'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-1-664748b5f142>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mNWORDS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'big.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0malphabet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'abcdefghijklmnopqrstuvwxyz'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'big.txt'"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Apparently, it fails, which is no surprise given that I don't have the *big.txt* data file he uses to construct his dictionary of possible words (train his model, as he says). Let's do this step by step. The file we're talking about is [online](http://norvig.com/big.txt), so I'm just gonna use a local version of it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%time NWORDS = train(words(file(r'files/big.txt').read()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 0.46 s, sys: 0.00 s, total: 0.46 s\n",
        "Wall time: 0.47 s\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Surprising how fast this was, given that this file is 6 MB and full of words. Let's check how many words actually end up in the dictionary."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(NWORDS.keys())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "29157"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How about sampling some of these words?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "NWORDS.keys()[:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "['nunnery',\n",
        " 'presnya',\n",
        " 'woods',\n",
        " 'clotted',\n",
        " 'spiders',\n",
        " 'hanging',\n",
        " 'disobeying',\n",
        " 'scold',\n",
        " 'originality',\n",
        " 'grenadiers',\n",
        " 'pigment',\n",
        " 'appropriation',\n",
        " 'strictest',\n",
        " 'bringing',\n",
        " 'revelers',\n",
        " 'wooded',\n",
        " 'wooden',\n",
        " 'wednesday',\n",
        " 'shows',\n",
        " 'immunities']"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Theory"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I'm going to try to understand the theory Norvig is explaining in this section. We will say that we are trying to find the correction $c$, out of all possible corrections, that maximizes the probability of $c$ given the original input word $w$:\n",
      "\n",
      "$$c_0 = \\argmax_c P(c | w)$$\n",
      "\n",
      "This expression can be expanded using Bayes' theorem, whose demonstration is pasted below as a reminder."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$P(A|B)=\\frac{P(A \\cap B)}{P(B)}, \\text{ if } P(B) \\neq 0, $$\n",
      "$$P(B|A) = \\frac{P(A \\cap B)}{P(A)}, \\text{ if } P(A) \\neq 0,$$\n",
      "$$\\implies P(A \\cap B) = P(A|B) P(B) = P(B|A)P(A),$$\n",
      "$$\\implies P(A|B) = \\frac{P(B|A) P(A)}{P(B)}, \\text{ if } P(B) \\neq 0.$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When we expand the orignal expression, we obtain:\n",
      "\n",
      "$$c_0 = \\argmax_c \\frac{P(w | c) P(c)}{P(w)}$$\n",
      "\n",
      "Assuming that all typos are equally probable for a given correct word, we can set $P(w)$ to 1 and thus ignore it. \n",
      "\n",
      "$$c_0 = \\argmax_c P(w | c) P(c)$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}