{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've always been interested by audio processing. At the recent SciPy 2015 conference, two talks involving \"live\" spectrograms were part of the highlights (the first concerning [Bokeh](https://www.youtube.com/watch?v=c9CgHHz_iYk&index=5&list=PLYx7XA2nY5Gcpabmu61kKcToLz0FapmHu), a great Python graphics backend for your browser and the second by the [VisPy team](https://www.youtube.com/watch?v=_3YoaeoiIFI&list=PLYx7XA2nY5Gcpabmu61kKcToLz0FapmHu&index=2), an impressive OpenGL Python rendering toolkit for science).\n",
    "This spiked my interest and I've decided to devote a little time to write a simple app for live audio processing.\n",
    "I first tried to start with the example from VisPy but as is part of a [pull request that's not yet merged into the main branch](https://github.com/vispy/vispy/pull/928), I decided to start from scratch using some of the code provided by the VisPy example and combine it with my existing knowledge of matplotlib and PyQt.\n",
    "\n",
    "The app I want to code should do the following things:\n",
    "\n",
    "- record a sound from my microphone continuously\n",
    "- plot the waveform and the spectrum (frequency representation) of the current waveform\n",
    "\n",
    "The final result can be seen on YouTube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/1XxR9U_aUog\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x104914c18>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('1XxR9U_aUog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This blog post works as follows: \n",
    "\n",
    "- we'll first look at the Microphone class that records sound and explain its behaviour\n",
    "- in a second step, we'll design a Qt application that plots the data from the microphone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuously recording sound from the microphone using PyAudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is actually a little bit tricky to record sound from the microphone with Python. In particular, I first tried doing this in pure PyQt, but failed. This seems to be possible, but given I did not manage to find code that does this on the internet, I abandoned this way of doing it. \n",
    "\n",
    "Inspired by the VisPy talk, I decided to take over their implementation of a continuously recording microphone using PyAudio. The PyAudio bindings are great because they allow you to use most devices on most platforms. On Mac, I haven't been able to install them into Python 3, but I managed to find a conda package compatible with Python 2.7. So the following code needs to run under Python 2.7 on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# class taken from the SciPy 2015 Vispy talk opening example \n",
    "# see https://github.com/vispy/vispy/pull/928\n",
    "import pyaudio\n",
    "import threading\n",
    "import atexit\n",
    "import numpy as np\n",
    "\n",
    "class MicrophoneRecorder(object):\n",
    "    def __init__(self, rate=4000, chunksize=1024):\n",
    "        self.rate = rate\n",
    "        self.chunksize = chunksize\n",
    "        self.p = pyaudio.PyAudio()\n",
    "        self.stream = self.p.open(format=pyaudio.paInt16,\n",
    "                                  channels=1,\n",
    "                                  rate=self.rate,\n",
    "                                  input=True,\n",
    "                                  frames_per_buffer=self.chunksize,\n",
    "                                  stream_callback=self.new_frame)\n",
    "        self.lock = threading.Lock()\n",
    "        self.stop = False\n",
    "        self.frames = []\n",
    "        atexit.register(self.close)\n",
    "\n",
    "    def new_frame(self, data, frame_count, time_info, status):\n",
    "        data = np.fromstring(data, 'int16')\n",
    "        with self.lock:\n",
    "            self.frames.append(data)\n",
    "            if self.stop:\n",
    "                return None, pyaudio.paComplete\n",
    "        return None, pyaudio.paContinue\n",
    "    \n",
    "    def get_frames(self):\n",
    "        with self.lock:\n",
    "            frames = self.frames\n",
    "            self.frames = []\n",
    "            return frames\n",
    "    \n",
    "    def start(self):\n",
    "        self.stream.start_stream()\n",
    "\n",
    "    def close(self):\n",
    "        with self.lock:\n",
    "            self.stop = True\n",
    "        self.stream.close()\n",
    "        self.p.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class works as a microphone recorder by wrapping functions provided by PyAudio (as shown in the [PyAudio examples](https://people.csail.mit.edu/hubert/pyaudio/#examples)). On instantiation it creates a PyAudio object that uses a callback to feed an internal buffer, `self.frames`, with audio chunks that always have the same length (as defined by `chunksize`). \n",
    "\n",
    "The internal buffer can then be accessed (and emptied) using `MicrophoneRecorder.get_frames`.\n",
    "\n",
    "One of the tricky things in this class is that it actually runs in a thread. So to make sure it stops correctly, a call to `atexit` is registered at startup. Unfortunately, this also makes the class unrunnable in the IPython notebook (so I won't instantiate it here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a mechanism for getting the audio, we can design the graphical part of the app: a GUI written using PyQt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The audio GUI written using PyQt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The audio GUI's function is easy to describe: it should fetch the accumulated output from the microphone recorder at regular intervals and plot the corresponding signal. The plotting should comprise both a time series and a frequency spectrum computed with `numpy.fft`.\n",
    "\n",
    "It turns out that the way I do the plooting was to use `matplotlib`. Although in general not suited for real-time graphical interfaces, it works in this case. The reason is that we don't redraw a whole figure at each call to the plotting but only lines that we have already instantiated. So in fact the updating is quick enough for our means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interface a matplotlib plot with PyQt is done using a specific class, whose code is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_qt4agg import FigureCanvasQTAgg as FigureCanvas\n",
    "from matplotlib.backends.backend_qt4agg import NavigationToolbar2QT as NavigationToolbar\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MplFigure(object):\n",
    "    def __init__(self, parent):\n",
    "        self.figure = plt.figure(facecolor='white')\n",
    "        self.canvas = FigureCanvas(self.figure)\n",
    "        self.toolbar = NavigationToolbar(self.canvas, parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This just creates an object that has a handle to a matplotlib canvas, which will be used to plot our stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous object is incorporated into a pretty classic user interface with a checkbox and a slider.\n",
    "\n",
    "- the checkbox allows for automatic scaling of the spectrum, which is useful if you're recording sounds at different amplitudes and not sure of your output beforehand\n",
    "- on the other hand, if you uncheck the box, you can control the normalization of the spectrum using a fixed gain provided by the slider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PyQt4 import QtGui, QtCore\n",
    "\n",
    "class LiveFFTWidget(QtGui.QWidget):\n",
    "    def __init__(self):\n",
    "        QtGui.QWidget.__init__(self)\n",
    "        \n",
    "        # customize the UI\n",
    "        self.initUI()\n",
    "        \n",
    "        # init class data\n",
    "        self.initData()       \n",
    "        \n",
    "        # connect slots\n",
    "        self.connectSlots()\n",
    "        \n",
    "        # init MPL widget\n",
    "        self.initMplWidget()\n",
    "        \n",
    "    def initUI(self):\n",
    "\n",
    "        hbox_gain = QtGui.QHBoxLayout()\n",
    "        autoGain = QtGui.QLabel('Auto gain for frequency spectrum')\n",
    "        autoGainCheckBox = QtGui.QCheckBox(checked=True)\n",
    "        hbox_gain.addWidget(autoGain)\n",
    "        hbox_gain.addWidget(autoGainCheckBox)\n",
    "        \n",
    "        # reference to checkbox\n",
    "        self.autoGainCheckBox = autoGainCheckBox\n",
    "        \n",
    "        hbox_fixedGain = QtGui.QHBoxLayout()\n",
    "        fixedGain = QtGui.QLabel('Manual gain level for frequency spectrum')\n",
    "        fixedGainSlider = QtGui.QSlider(QtCore.Qt.Horizontal)\n",
    "        hbox_fixedGain.addWidget(fixedGain)\n",
    "        hbox_fixedGain.addWidget(fixedGainSlider)\n",
    "\n",
    "        self.fixedGainSlider = fixedGainSlider\n",
    "\n",
    "        vbox = QtGui.QVBoxLayout()\n",
    "\n",
    "        vbox.addLayout(hbox_gain)\n",
    "        vbox.addLayout(hbox_fixedGain)\n",
    "\n",
    "        # mpl figure\n",
    "        self.main_figure = MplFigure(self)\n",
    "        vbox.addWidget(self.main_figure.toolbar)\n",
    "        vbox.addWidget(self.main_figure.canvas)\n",
    "        \n",
    "        self.setLayout(vbox) \n",
    "        \n",
    "        self.setGeometry(300, 300, 350, 300)\n",
    "        self.setWindowTitle('LiveFFT')    \n",
    "        self.show()\n",
    "        # timer for callbacks, taken from:\n",
    "        # http://ralsina.me/weblog/posts/BB974.html\n",
    "        timer = QtCore.QTimer()\n",
    "        timer.timeout.connect(self.handleNewData)\n",
    "        timer.start(100)\n",
    "        # keep reference to timer        \n",
    "        self.timer = timer\n",
    "        \n",
    "     \n",
    "    def initData(self):\n",
    "        mic = MicrophoneRecorder()\n",
    "        mic.start()  \n",
    "\n",
    "        # keeps reference to mic        \n",
    "        self.mic = mic\n",
    "        \n",
    "        # computes the parameters that will be used during plotting\n",
    "        self.freq_vect = np.fft.rfftfreq(mic.chunksize, \n",
    "                                         1./mic.rate)\n",
    "        self.time_vect = np.arange(mic.chunksize, dtype=np.float32) / mic.rate * 1000\n",
    "                \n",
    "    def connectSlots(self):\n",
    "        pass\n",
    "    \n",
    "    def initMplWidget(self):\n",
    "        \"\"\"creates initial matplotlib plots in the main window and keeps \n",
    "        references for further use\"\"\"\n",
    "        # top plot\n",
    "        self.ax_top = self.main_figure.figure.add_subplot(211)\n",
    "        self.ax_top.set_ylim(-32768, 32768)\n",
    "        self.ax_top.set_xlim(0, self.time_vect.max())\n",
    "        self.ax_top.set_xlabel(u'time (ms)', fontsize=6)\n",
    "\n",
    "        # bottom plot\n",
    "        self.ax_bottom = self.main_figure.figure.add_subplot(212)\n",
    "        self.ax_bottom.set_ylim(0, 1)\n",
    "        self.ax_bottom.set_xlim(0, self.freq_vect.max())\n",
    "        self.ax_bottom.set_xlabel(u'frequency (Hz)', fontsize=6)\n",
    "        # line objects        \n",
    "        self.line_top, = self.ax_top.plot(self.time_vect, \n",
    "                                         np.ones_like(self.time_vect))\n",
    "        \n",
    "        self.line_bottom, = self.ax_bottom.plot(self.freq_vect,\n",
    "                                               np.ones_like(self.freq_vect))\n",
    "                                               \n",
    "\n",
    "                                               \n",
    "    def handleNewData(self):\n",
    "        \"\"\" handles the asynchroneously collected sound chunks \"\"\"        \n",
    "        # gets the latest frames        \n",
    "        frames = self.mic.get_frames()\n",
    "        \n",
    "        if len(frames) > 0:\n",
    "            # keeps only the last frame\n",
    "            current_frame = frames[-1]\n",
    "            # plots the time signal\n",
    "            self.line_top.set_data(self.time_vect, current_frame)\n",
    "            # computes and plots the fft signal            \n",
    "            fft_frame = np.fft.rfft(current_frame)\n",
    "            if self.autoGainCheckBox.checkState() == QtCore.Qt.Checked:\n",
    "                fft_frame /= np.abs(fft_frame).max()\n",
    "            else:\n",
    "                fft_frame *= (1 + self.fixedGainSlider.value()) / 5000000.\n",
    "                #print(np.abs(fft_frame).max())\n",
    "            self.line_bottom.set_data(self.freq_vect, np.abs(fft_frame))            \n",
    "            \n",
    "            # refreshes the plots\n",
    "            self.main_figure.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The refreshing part of the app is handled with a `QTimer` that gets called 10 times a second and refreshes the gui at that time by calling the `handleNewData` function. That function gets the latest frame from the microphone, plots the time series, computes the Fourier transform and plots its modulus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole app can be run using the following commands:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import sys\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = QtGui.QApplication(sys.argv)\n",
    "    window = LiveFFTWidget()\n",
    "    sys.exit(app.exec_())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I made a little video from the app working live on YouTube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/1XxR9U_aUog\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x104914be0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YouTubeVideo('1XxR9U_aUog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, we learned how to code a little app for live sound recording using PyAudio, a little audio processing using numpy and displaying the data using matplotlib embedded in the PyQt framework."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
